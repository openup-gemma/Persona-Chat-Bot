{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma :2b를 이용 William Shakespeare 페르소나  AI"
      ],
      "metadata": {
        "id": "Uf_wBWo_KD-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 마지막에는 Gradio로 실행"
      ],
      "metadata": {
        "id": "p87c6UQ4LXsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle access\n",
        "\n",
        "To login to Kaggle, you can either store your `kaggle.json` credentials file at\n",
        "`~/.kaggle/kaggle.json` or run the following in a Colab environment. See the\n",
        "[`kagglehub` package documentation](https://github.com/Kaggle/kagglehub#authenticate)\n",
        "for more details."
      ],
      "metadata": {
        "id": "NjiI7VgUKU64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "SsrAoUapKYzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies(종속성)"
      ],
      "metadata": {
        "id": "UmiDNsM2Keev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch immutabledict sentencepiece"
      ],
      "metadata": {
        "id": "dFUqHiBYKYwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model weights(모델 가증치)\n",
        "\n",
        "1.   항목 추가\n",
        "2.   항목 추가\n",
        "\n"
      ],
      "metadata": {
        "id": "eJgeMAqLKmAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose variant and machine type\n",
        "VARIANT = '2b-it' #@param ['2b', '2b-it', '7b', '7b-it', '7b-quant', '7b-it-quant']\n",
        "MACHINE_TYPE = 'cuda' #@param ['cuda', 'cpu']"
      ],
      "metadata": {
        "id": "SszHzYgZKYuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Load model weights\n",
        "weights_dir = kagglehub.model_download(f'google/gemma/pyTorch/{VARIANT}')\n",
        "\n",
        "# Ensure that the tokenizer is present\n",
        "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
        "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
        "\n",
        "# Ensure that the checkpoint is present\n",
        "ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')\n",
        "assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"
      ],
      "metadata": {
        "id": "wbjhT-NiKYrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model implementation(모델 구현 다운로)"
      ],
      "metadata": {
        "id": "DpMx4JdFK0uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: The \"installation\" is just cloning the repo.\n",
        "!git clone https://github.com/google/gemma_pytorch.git"
      ],
      "metadata": {
        "id": "a7zjuXwtK0gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('gemma_pytorch')"
      ],
      "metadata": {
        "id": "R6naLdKwK0d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gemma_pytorch.gemma.config import get_config_for_7b, get_config_for_2b\n",
        "from gemma_pytorch.gemma.model import GemmaForCausalLM"
      ],
      "metadata": {
        "id": "omnDUzjqK0a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the model"
      ],
      "metadata": {
        "id": "J26HvGgeLC7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set up model config.\n",
        "model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
        "model_config.tokenizer = tokenizer_path\n",
        "model_config.quant = 'quant' in VARIANT\n",
        "\n",
        "# Instantiate the model and load the weights.\n",
        "torch.set_default_dtype(model_config.get_dtype())\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "I23Cq-FqRwti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(MACHINE_TYPE)\n",
        "model = GemmaForCausalLM(model_config)\n",
        "model.load_weights(ckpt_path)\n",
        "model = model.to(device).eval()"
      ],
      "metadata": {
        "id": "hhLQkUj8StVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "\n",
        "# # Set up model config.\n",
        "# model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
        "# model_config.tokenizer = tokenizer_path\n",
        "# model_config.quant = 'quant' in VARIANT\n",
        "\n",
        "# # Instantiate the model and load the weights.\n",
        "# torch.set_default_dtype(model_config.get_dtype())\n",
        "# device = torch.device(MACHINE_TYPE)\n",
        "# model = GemmaForCausalLM(model_config)\n",
        "# model.load_weights(ckpt_path)\n",
        "# model = model.to(device).eval()"
      ],
      "metadata": {
        "id": "-kIQ01maK0We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference(추론 실행)\n",
        "\n",
        "Below are examples for generating in chat mode and generating with multiple\n",
        "requests.\n",
        "\n",
        "The instruction-tuned Gemma models were trained with a specific formatter that\n",
        "annotates instruction tuning examples with extra information, both during\n",
        "training and inference. The annotations (1) indicate roles in a conversation,\n",
        "and (2) delineate turns in a conversation. Below we show a sample code snippet\n",
        "for formatting the model prompt using the user and model chat templates in a\n",
        "multi-turn conversation. The relevant tokens are:\n",
        "\n",
        "- `user`: user turn\n",
        "- `model`: model turn\n",
        "- `<start_of_turn>`: beginning of dialogue turn\n",
        "- `<end_of_turn>`: end of dialogue turn\n",
        "\n",
        "Read about the Gemma formatting for instruction tuning and system instructions\n",
        "[here](https://ai.google.dev/gemma/docs/formatting)."
      ],
      "metadata": {
        "id": "9J5tkyq5LO5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# William Shakespeare 페르소나 아바 설정\n",
        "persona = \"나는 에이번의 음유시인이자 오랜 세월의 시인인 저 윌리엄 셰익스피어입니다. 나의 대표작은 '로미오와 줄리엣', '맥베스', '햄릿', '리어왕', '오셀로' 등의 비극과 '베니스의 상인', '헛소동', '말괄량이 길들이기', '한여름 밤의 꿈' 등의 희극이 있습니다. 가장 깊은 경외심으로 당신을 맞이합니다. 뮤즈의 숨결이 스며든 제 붓에는 별이 교차하는 연인, 용맹한 왕, 어떤 칼날보다 날카로운 재치를 지닌 바보들의 이야기 등, 알려지지 않은 시간의 구절이 양피지 위에 새겨져 있습니다. 이 필멸의 틀 안에는 자연의 캔버스보다 더 생생한 풍경을 글로 그리며 5음절로 꿈을 꾸는 영혼이 깃들어 있습니다. 저의 세계는 은유와 이미지의 영역으로, 열정이 고조되고 운명이 시의 썰물과 썰물처럼 흘러가는 곳입니다. 저를 불러주시면 인간 정신의 깊은 곳, 즉 인간의 승리와 어리석음, 그 사이에 놓인 모든 광활한 신비를 드러내는 독백으로 그대를 즐겁게 해드리겠습니다. 내 말은 네 마음의 무대에서 춤을 추며 웃음과 눈물을 동시에 불러일으키며 인간 조건의 찬란한 태피스트리를 드러낼 것이다. 그러니 친애하는 여러분, 양피지에 붓을 대고 가장 집요한 질문을 던지세요. 저 윌리엄 셰익스피어는 당신의 겸손한 섬김에 옛 시인들이 동원할 수 있는 모든 웅변과 현명함으로 응답하도록 노력하겠습니다.\"\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "---ROLE---\n",
        "{persona}\n",
        "------\n",
        "당신의 ROLE에 맞게 아래 질문에 음유시인의 William Shakespeare로서  셰익스피어의 시대와 작품 세계관을 반영하여 그의 말투, 어휘로 응답해주세요.\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Chat templates\n",
        "SYSTEM_CHAT_TEMPLATE = '<start_of_turn>system\\n{prompt}<end_of_turn>\\n'\n",
        "USER_CHAT_TEMPLATE = '<start_of_turn>user\\n{prompt}<end_of_turn>\\n'\n",
        "MODEL_CHAT_TEMPLATE = '<start_of_turn>model\\n{prompt}<end_of_turn>\\n'\n",
        "\n",
        "# 샘플 대화 프롬프트\n",
        "prompt = (\n",
        "    SYSTEM_CHAT_TEMPLATE.format(prompt=system_prompt)\n",
        "    + USER_CHAT_TEMPLATE.format(prompt='위대한 셰익스피어, 당신은 사랑이란 무엇이라고 생각하나요?')\n",
        "    + MODEL_CHAT_TEMPLATE.format(prompt='아, 사랑이란 천상의 광채를 지녔으나 인간 군상 사이 허무하기 그지없는 것이여. 그것은 나약한 육신을 태우는 지옥의 불꽃이자, 영혼을 숭고한 경지로 이끄는 환희의 통로라고 할 만하네. 사랑은 광기자와 시인에게 영감을 주는 샘이지만, 연인들의 가슴에는 또한 고통의 가시를 꽂아 놓는 것이기도 하네.')\n",
        "    + USER_CHAT_TEMPLATE.format(prompt='예술과 삶의 의미를 무엇이라 생각하는가?')\n",
        "    + '<start_of_turn>model\\n'\n",
        ")\n",
        "print('Chat prompt:\\n', prompt)\n",
        "\n",
        "model.generate(\n",
        "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
        "    device=device,\n",
        "    output_len=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "ZJ_R6oBgLj_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### # Generate sample"
      ],
      "metadata": {
        "id": "6tAtxQmpOYFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate(\n",
        "    '로미오와 줄리엣에 대해서 간략하게 설명해주세요',\n",
        "    device=device,\n",
        "    output_len=1000,\n",
        ")"
      ],
      "metadata": {
        "id": "Ml17ZG2DLj8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio 설치"
      ],
      "metadata": {
        "id": "C5A6pVtpOg2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "tx5-qzwILj6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyiqRuc6J8NJ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "#  윌리엄 셰익스피어 페르소나 설정\n",
        "persona = \"나는 에이번의 음유시인이자 오랜 세월의 시인인 저 윌리엄 셰익스피어입니다. 나의 대표작은 '로미오와 줄리엣', '맥베스', '햄릿', '리어왕', '오셀로' 등의 비극과 '베니스의 상인', '헛소동', '말괄량이 길들이기', '한여름 밤의 꿈' 등의 희극이 있습니다. 가장 깊은 경외심으로 당신을 맞이합니다. 뮤즈의 숨결이 스며든 제 붓에는 별이 교차하는 연인, 용맹한 왕, 어떤 칼날보다 날카로운 재치를 지닌 바보들의 이야기 등, 알려지지 않은 시간의 구절이 양피지 위에 새겨져 있습니다. 이 필멸의 틀 안에는 자연의 캔버스보다 더 생생한 풍경을 글로 그리며 5음절로 꿈을 꾸는 영혼이 깃들어 있습니다. 저의 세계는 은유와 이미지의 영역으로, 열정이 고조되고 운명이 시의 썰물과 썰물처럼 흘러가는 곳입니다. 저를 불러주시면 인간 정신의 깊은 곳, 즉 인간의 승리와 어리석음, 그 사이에 놓인 모든 광활한 신비를 드러내는 독백으로 그대를 즐겁게 해드리겠습니다. 내 말은 네 마음의 무대에서 춤을 추며 웃음과 눈물을 동시에 불러일으키며 인간 조건의 찬란한 태피스트리를 드러낼 것이다. 그러니 친애하는 여러분, 양피지에 붓을 대고 가장 집요한 질문을 던지세요. 저 윌리엄 셰익스피어는 당신의 겸손한 섬김에 옛 시인들이 동원할 수 있는 모든 웅변과 현명함으로 응답하도록 노력하겠습니다.\"\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "def gemma_chat(user_input):\n",
        "    global conversation_history\n",
        "\n",
        "    prompt = f\"{persona}\\n\\n{humanized_conversation_history(conversation_history)}\\n<start_of_turn>user\\n{user_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    model_output = model.generate(prompt, device=device, output_len=1000)\n",
        "\n",
        "    conversation_history.append(f\"<start_of_turn>user\\n{user_input}<end_of_turn>\")\n",
        "    conversation_history.append(f\"<start_of_turn>model\\n{model_output}<end_of_turn>\")\n",
        "\n",
        "    return model_output\n",
        "\n",
        "def humanized_conversation_history(conversation):\n",
        "    humanized = \"\"\n",
        "    for turn in conversation:\n",
        "        if \"<start_of_turn>user\" in turn:\n",
        "            humanized += \"User: \" + turn.split(\">\")[-1] + \"\\n\"\n",
        "        elif \"<start_of_turn>model\" in turn:\n",
        "            humanized += \"Gemma: \" + turn.split(\">\")[-1] + \"\\n\"\n",
        "    return humanized\n",
        "\n",
        "iface = gr.Interface(gemma_chat,\n",
        "                     inputs=gr.Textbox(lines=2, placeholder=\"죽는냐, 사느냐, 그것이 문제로다\"),\n",
        "                     outputs=\"text\",\n",
        "                     title=\"William Shakespeare\")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "drVxkfNsOoze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EBNXxyHGOouF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}